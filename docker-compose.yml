version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks: [data-network]

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "kafka:9092", "--list"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks: [data-network]

  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks: [data-network]

  producer:
    build: ./producer
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    networks: [data-network]

  consumer:
    build: ./consumer
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    networks: [data-network]

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    networks: [data-network]

  airflow-init:
      image: apache/airflow:2.6.3
      container_name: airflow-init
      depends_on:
        - postgres
      environment:
        AIRFLOW__CORE__EXECUTOR: SequentialExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
        AIRFLOW__WEBSERVER__SECRET_KEY: airflow_secret_2026
      command: >
        bash -c "
        echo 'Waiting for postgres...' &&
        until pg_isready -h postgres -p 5432; do sleep 2; done &&
        airflow db upgrade &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@test.com
        "
      networks: [data-network]

  airflow:
    image: apache/airflow:2.6.3
    container_name: airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__WEBSERVER__SECRET_KEY: airflow_secret_2026
    command: webserver
    ports:
      - "8003:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    networks: [data-network]


  airflow-scheduler:
    image: apache/airflow:2.6.3
    container_name: airflow-scheduler
    restart: unless-stopped
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: airflow_secret_2026
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    networks:
      - data-network
  spark-master:
    image: apache/spark:3.5.2
    container_name: spark-master
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.master.Master
    ports:
      - "8081:8080"
      - "7077:7077"
    volumes:
      - spark_ivy:/home/spark/.ivy2
      - ./spark/jobs:/opt/spark/jobs
    networks: [data-network]


  spark-worker:
    image: apache/spark:3.5.2
    container_name: spark-worker
    depends_on:
      - spark-master
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - spark://spark-master:7077
    volumes:
      - spark_ivy:/home/spark/.ivy2
    networks: [data-network]
  adminer:
    image: adminer
    container_name: adminer
    ports:
      - "8082:8080"
    depends_on:
      - postgres
    networks: [data-network]

volumes:
  minio_data:
  pgdata:
  spark_ivy:

networks:
  data-network: